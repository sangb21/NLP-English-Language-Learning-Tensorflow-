{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa LLRD + LastLayerReinit with TensorFlow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Fold specific strategy**\n",
    "\n",
    "|Ver |Fold (seed)| lr_head/lr_base| decay| layer-decay|decay_steps|maxlen| CV |LB|Strategies|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|5| 0-4 (42)| 1e-4/.9e-5 |0.3|**1.0/.9**|5*len(ds)|512|.452/.464/.468/.456/.459|.43|4-Layer Weighted-Pool + Reinit|\n",
    "|6| 0-4 (42)| 1e-4/.9e-5 |0.3|**1.0/.9**|5*len(ds)|600|.451/.464/.467/.455/.458|.43|4-Layer Weighted-Pool + Reinit|\n",
    "|| 0/1/2/3/4 (42)| 1e-4/.9e-5 |0.3|1/.9|5*len(ds)|800|.452/.460/.465/.461/.460| .44|reinit+4-l-weight|\n",
    "\n",
    "**Layer-wise LR + Weighted MeanPool**\n",
    "\n",
    "|Ver |Fold (seed)| lr_head/base| decay| layer-decay|decay_steps|maxlen| CV |LB|Remarks|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|16| 0-4 (42)| 1e-4/.9e-5 |0.3|0.3/.5|5*len(ds)|512|.451/.461/.462/**.450**/**.459**|.44|**BEST LB**|\n",
    "|1| 0-4 (42)| 1e-4/.9e-5 |0.3|**1.0/.5**|5*len(ds)|512|.452/.459/.465/.456/.458|.44|Mean Pool|\n",
    "|2| 0-4 (42)| 1e-4/.9e-5 |0.3|**1.0/.5**|5*len(ds)|512|.454/.461/.465/.455/.459||4-Layer Weighted-Pool|\n",
    "|3| 0-4 (42)| 1e-4/.9e-5 |0.3|**1.0/.5**|5*len(ds)|512|.454/.462/.467/.458/.457||4-Layer Weighted-Pool + Reinit|\n",
    "|4| 0-4 (42)| 1e-4/.9e-5 |0.3|**1.0/.7**|5*len(ds)|512|.451/.461/.464/.457/.457|.44|4-Layer Weighted-Pool + Reinit|\n",
    "|5| 0-4 (42)| 1e-4/.9e-5 |0.3|**1.0/.9**|5*len(ds)|512|.452/.464/.468/.456/.459|.43|4-Layer Weighted-Pool + Reinit|\n",
    "|6| 0-4 (42)| 1e-4/.9e-5 |0.3|**1.0/.9**|5*len(ds)|600|.451/.464/.467/.455/.458|.43|4-Layer Weighted-Pool + Reinit|\n",
    "|REF|0-4 (42)| 1e-4/1.e-5 |0.3|1.0/.9|5*len(ds)|512|**.450/.459/.462/.455/.452**|.43|Zhang chang| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialize strategy for  GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    strategy = tf.distribute.MirroredStrategy() \n",
    "else: \n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    \n",
    "#tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "\n",
    "# USE AUTO-MIXED PRECISION\n",
    "#tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "# https://keras.io/api/mixed_precision/loss_scale_optimizer/\n",
    "\n",
    "print('Mixed precision enabled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:13:21.402329Z",
     "iopub.status.busy": "2022-11-18T16:13:21.401587Z",
     "iopub.status.idle": "2022-11-18T16:13:28.870929Z",
     "shell.execute_reply": "2022-11-18T16:13:28.86972Z",
     "shell.execute_reply.started": "2022-11-18T16:13:21.402127Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f'TF version: {tf.__version__}')\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import transformers\n",
    "print(f'transformers version: {transformers.__version__}')\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/iterative-stratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:13:28.87404Z",
     "iopub.status.busy": "2022-11-18T16:13:28.873408Z",
     "iopub.status.idle": "2022-11-18T16:13:28.880377Z",
     "shell.execute_reply": "2022-11-18T16:13:28.878767Z",
     "shell.execute_reply.started": "2022-11-18T16:13:28.874004Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Hyperparameters():\n",
    "    \n",
    "    RUN = 6\n",
    "    \n",
    "    EPOCHS = 5   \n",
    "    \n",
    "    TRAIN_FOLD = (0, 5)\n",
    "    \n",
    "    N_FOLDS = 5\n",
    "    BATCH_SIZE = 4\n",
    "    MAX_LENGTH = 600\n",
    "    \n",
    "    LLRDR = 0.9                 # Layerwise LR Decay Rate\n",
    "    INIT_LR = 0.9e-5             # Initial LR of Basemodel\n",
    "    INITIAL_LR_HEAD = 1e-4\n",
    "    \n",
    "    #pool_types = {0: 'mean', 1: 'mean', 2: 'mean', 3: 'mean', 4: 'mean'}\n",
    "    #pool_types = {0: 'weighted-mean-2', 1: 'weighted-mean-2', 2: 'weighted-mean-2', 3: 'weighted-mean-2', 4: 'weighted-mean-2'}\n",
    "    pool_types = {0: 'weighted-mean-4', 1: 'weighted-mean-4', 2: 'weighted-mean-4', 3: 'weighted-mean-4', 4: 'weighted-mean-4'}\n",
    "    \n",
    "    #reinit_last_layers = {0:False, 1: False, 2: False, 3: False, 4: False}\n",
    "    reinit_last_layers = {0: True, 1: True, 2: True, 3: True, 4: True}\n",
    "        \n",
    "    DEBERTA_MODEL = \"../input/tf-deberta-v3-base/model\" #\"../input/debertav3base\"\n",
    "    \n",
    "    #pretrained_model_path = '../input/tf-deberta-v3-base/model'    \n",
    "    #pretrained_model_path = '../input/tf-bart-base/model'\n",
    "    #pretrained_model_path = '../input/tf-roberta-base/model'\n",
    "\n",
    "    \n",
    "    model_weight_name = f'fb3-train-v5-run{RUN}'\n",
    "    \n",
    "hp = Hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:13:28.883027Z",
     "iopub.status.busy": "2022-11-18T16:13:28.882361Z",
     "iopub.status.idle": "2022-11-18T16:13:29.178303Z",
     "shell.execute_reply": "2022-11-18T16:13:29.177016Z",
     "shell.execute_reply.started": "2022-11-18T16:13:28.882992Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/feedback-prize-english-language-learning/train.csv')\n",
    "display(df.head())\n",
    "print('\\n---------DataFrame Summary---------')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:13:29.179855Z",
     "iopub.status.busy": "2022-11-18T16:13:29.179504Z",
     "iopub.status.idle": "2022-11-18T16:13:29.336365Z",
     "shell.execute_reply": "2022-11-18T16:13:29.334996Z",
     "shell.execute_reply.started": "2022-11-18T16:13:29.179822Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_COLS = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "\n",
    "skf = MultilabelStratifiedKFold(n_splits=hp.N_FOLDS, shuffle=True, random_state=42)\n",
    "for n, (train_index, val_index) in enumerate(skf.split(df, df[TARGET_COLS])):\n",
    "    df.loc[val_index, 'fold'] = int(n)\n",
    "df['fold'] = df['fold'].astype(int)\n",
    "\n",
    "df['fold'].value_counts()\n",
    "\n",
    "df.to_csv('./df_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOkenizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:13:29.553281Z",
     "iopub.status.busy": "2022-11-18T16:13:29.55288Z",
     "iopub.status.idle": "2022-11-18T16:13:31.533885Z",
     "shell.execute_reply": "2022-11-18T16:13:31.53302Z",
     "shell.execute_reply.started": "2022-11-18T16:13:29.553247Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(hp.DEBERTA_MODEL)\n",
    "#tokenizer.save_pretrained('./tokenizer/')\n",
    "\n",
    "#cfg = transformers.AutoConfig.from_pretrained(hp.DEBERTA_MODEL, output_hidden_states=True)\n",
    "#cfg.hidden_dropout_prob = 0\n",
    "#cfg.attention_probs_dropout_prob = 0\n",
    "#cfg.save_pretrained('./tokenizer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:13:31.535542Z",
     "iopub.status.busy": "2022-11-18T16:13:31.535182Z",
     "iopub.status.idle": "2022-11-18T16:13:31.543204Z",
     "shell.execute_reply": "2022-11-18T16:13:31.542011Z",
     "shell.execute_reply.started": "2022-11-18T16:13:31.53551Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deberta_encode(texts, tokenizer=tokenizer):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for text in texts.tolist():\n",
    "        token = tokenizer(text, \n",
    "                          add_special_tokens=True, \n",
    "                          max_length=hp.MAX_LENGTH, \n",
    "                          return_attention_mask=True, \n",
    "                          return_tensors=\"np\", \n",
    "                          truncation=True, \n",
    "                          padding='max_length')\n",
    "        input_ids.append(token['input_ids'][0])\n",
    "        attention_mask.append(token['attention_mask'][0])\n",
    "    \n",
    "    return np.array(input_ids, dtype=\"int32\"), np.array(attention_mask, dtype=\"int32\")\n",
    "\n",
    "\n",
    "def get_dataset(df):\n",
    "    inputs = deberta_encode(df['full_text'])\n",
    "    targets = np.array(df[TARGET_COLS], dtype=\"float32\")\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:13:31.545356Z",
     "iopub.status.busy": "2022-11-18T16:13:31.544817Z",
     "iopub.status.idle": "2022-11-18T16:13:31.578394Z",
     "shell.execute_reply": "2022-11-18T16:13:31.577278Z",
     "shell.execute_reply.started": "2022-11-18T16:13:31.545313Z"
    },
    "tags": []
   },
   "source": [
    "## MeanPool and Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "## MeanPool\n",
    "class MeanPool(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, mask=None):\n",
    "        broadcast_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
    "        embedding_sum = tf.reduce_sum(inputs * broadcast_mask, axis=1)\n",
    "        mask_sum = tf.reduce_sum(broadcast_mask, axis=1)\n",
    "        mask_sum = tf.math.maximum(mask_sum, tf.constant([1e-9]))\n",
    "        return embedding_sum / mask_sum\n",
    "        \n",
    "## WeightedLayerPool\n",
    "class WeightsSumOne(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        return tf.nn.softmax(w, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:13:31.640919Z",
     "iopub.status.busy": "2022-11-18T16:13:31.640049Z",
     "iopub.status.idle": "2022-11-18T16:13:31.650104Z",
     "shell.execute_reply": "2022-11-18T16:13:31.649309Z",
     "shell.execute_reply.started": "2022-11-18T16:13:31.64087Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MSE(y_true, y_pred):\n",
    "\n",
    "    weights = tf.constant(\n",
    "        [1., 1., 1., 1., 1., 1.], dtype=tf.float32\n",
    "    )\n",
    "    weighted_sq_error = tf.math.multiply(\n",
    "        weights, tf.square(y_true - y_pred)\n",
    "    )\n",
    "    return tf.reduce_mean(weighted_sq_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:13:31.653625Z",
     "iopub.status.busy": "2022-11-18T16:13:31.652166Z",
     "iopub.status.idle": "2022-11-18T16:13:31.671776Z",
     "shell.execute_reply": "2022-11-18T16:13:31.670857Z",
     "shell.execute_reply.started": "2022-11-18T16:13:31.653437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(hp, fold, len_train_df=3129):\n",
    "    \n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(hp.MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\"\n",
    "    )\n",
    "    attention_masks = tf.keras.layers.Input(\n",
    "        shape=(hp.MAX_LENGTH,), dtype=tf.int32, name=\"attention_masks\"\n",
    "    )\n",
    "\n",
    "    cfg = transformers.AutoConfig.from_pretrained(hp.DEBERTA_MODEL, output_hidden_states=True)\n",
    "    base_model = transformers.TFAutoModel.from_pretrained(hp.DEBERTA_MODEL, config=cfg)\n",
    "    \n",
    "    #Last Layer Reinitialization or Partially Reinitialization\n",
    "    #Uncommon next three lines to check deberta encoder block\n",
    "    #print('DeBERTa Encoder Block:')\n",
    "    #for layer in base_model.deberta.encoder.layer:\n",
    "    #print(layer)\n",
    "\n",
    "    if hp.reinit_last_layers[fold]:\n",
    "        REINIT_LAYERS = 1\n",
    "        normal_initializer = tf.keras.initializers.GlorotUniform()\n",
    "        zeros_initializer = tf.keras.initializers.Zeros()\n",
    "        ones_initializer = tf.keras.initializers.Ones()\n",
    "\n",
    "        #     print(f'\\nRe-initializing encoder block:')\n",
    "        for encoder_block in base_model.deberta.encoder.layer[-REINIT_LAYERS:]:\n",
    "            #         print(f'{encoder_block}')\n",
    "            for layer in encoder_block.submodules:\n",
    "                if isinstance(layer, tf.keras.layers.Dense):\n",
    "                    layer.kernel.assign(normal_initializer(shape=layer.kernel.shape, dtype=layer.kernel.dtype))\n",
    "                    if layer.bias is not None:\n",
    "                        layer.bias.assign(zeros_initializer(shape=layer.bias.shape, dtype=layer.bias.dtype))\n",
    "\n",
    "                elif isinstance(layer, tf.keras.layers.LayerNormalization):\n",
    "                    layer.beta.assign(zeros_initializer(shape=layer.beta.shape, dtype=layer.beta.dtype))\n",
    "                    layer.gamma.assign(ones_initializer(shape=layer.gamma.shape, dtype=layer.gamma.dtype))\n",
    "                    \n",
    "    if hp.pool_types[fold] == 'weighted-mean-4':\n",
    "        # WeightedLayerPool + MeanPool of the last 4 hidden states\n",
    "        deberta_output = base_model.deberta(\n",
    "            input_ids, attention_mask=attention_masks\n",
    "        )\n",
    "        hidden_states = deberta_output.hidden_states\n",
    "        stack_meanpool = tf.stack(\n",
    "            [MeanPool()(hidden_s, mask=attention_masks) for hidden_s in hidden_states[-4:]], \n",
    "            axis=2)\n",
    "        weighted_layer_pool = layers.Dense(1,use_bias=False, kernel_constraint=WeightsSumOne())(stack_meanpool)\n",
    "        weighted_layer_pool = tf.squeeze(weighted_layer_pool, axis=-1)\n",
    "        output = layers.Dense(6)(weighted_layer_pool)\n",
    "        # New variable - trainable_head_layers\n",
    "        trainable_head_layers = 4\n",
    "    \n",
    "\n",
    "    if hp.pool_types[fold] == 'mean':\n",
    "        # Mean Pool Only\n",
    "        x = base_model.deberta(input_ids, attention_mask=attention_masks)[0]\n",
    "        x = MeanPool()(x, mask=attention_masks)\n",
    "        output = layers.Dense(6)(x)\n",
    "        trainable_head_layers = 2\n",
    "    \n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_ids, attention_masks], \n",
    "        outputs=output\n",
    "    )\n",
    "   \n",
    "    print('Last 4 layers of the model: ')\n",
    "    print(model.layers[-4:])\n",
    "    print('')\n",
    "    \n",
    "    #Compile model with Layer-wise Learning Rate Decay\n",
    "    layer_list = [base_model.deberta.embeddings] + list(base_model.deberta.encoder.layer)\n",
    "    layer_list.reverse()\n",
    "    \n",
    "    LR_SCH_DECAY_STEPS =  hp.EPOCHS * len_train_df //  hp.BATCH_SIZE\n",
    "    #LR_SCH_DECAY_STEPS =  1600 #hp.EPOCHS * len_train_df //  hp.BATCH_SIZE\n",
    "    \n",
    "    lr_schedules = [tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=hp.INIT_LR * hp.LLRDR ** i, \n",
    "        decay_steps=LR_SCH_DECAY_STEPS, \n",
    "        decay_rate=0.3) for i in range(len(layer_list))]\n",
    "    \n",
    "    lr_schedule_head = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=hp.INITIAL_LR_HEAD, \n",
    "        decay_steps=LR_SCH_DECAY_STEPS, \n",
    "        decay_rate=0.3)\n",
    "    \n",
    "    optimizers = [tf.keras.optimizers.Adam(learning_rate=lr_sch) for lr_sch in lr_schedules]\n",
    "    \n",
    "    optimizers_and_layers = [\n",
    "        (tf.keras.optimizers.Adam(learning_rate=lr_schedule_head), model.layers[-trainable_head_layers:])\n",
    "    ] +\\\n",
    "        list(zip(optimizers, layer_list))\n",
    "    \n",
    "    #     Uncomment next three lines to check optimizers_and_layers\n",
    "    #     print('\\nLayer-wise Learning Rate Decay Initial LR:')\n",
    "    #     for o,l in optimizers_and_layers:\n",
    "    #         print(f'{o._decayed_lr(\"float32\").numpy()} for {l}')\n",
    "\n",
    "    optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)   \n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss=MSE, #'huber_loss',\n",
    "                 metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "                 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Folds Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-18T16:19:22.391397Z",
     "iopub.status.busy": "2022-11-18T16:19:22.390899Z",
     "iopub.status.idle": "2022-11-18T16:21:46.55373Z",
     "shell.execute_reply": "2022-11-18T16:21:46.552225Z",
     "shell.execute_reply.started": "2022-11-18T16:19:22.391358Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_rmses = []\n",
    "\n",
    "#for fold in range(hp.N_FOLDS):\n",
    "for fold in range(hp.TRAIN_FOLD[0], hp.TRAIN_FOLD[1]):\n",
    "    \n",
    "    \n",
    "    print(f'\\n-----------FOLD {fold} ------------')\n",
    "    \n",
    "    #Create dataset\n",
    "    train_df = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    valid_df = df[df['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = get_dataset(train_df)\n",
    "    valid_dataset = get_dataset(valid_df)\n",
    "    \n",
    "    print('Data prepared.')\n",
    "    print(f'Training data input_ids shape: {train_dataset[0][0].shape} dtype: {train_dataset[0][0].dtype}') \n",
    "    print(f'Training data attention_mask shape: {train_dataset[0][1].shape} dtype: {train_dataset[0][1].dtype}')\n",
    "    print(f'Training data targets shape: {train_dataset[1].shape} dtype: {train_dataset[1].dtype}')\n",
    "    print(f'Validation data input_ids shape: {valid_dataset[0][0].shape} dtype: {valid_dataset[0][0].dtype}')\n",
    "    print(f'Validation data attention_mask shape: {valid_dataset[0][1].shape} dtype: {valid_dataset[0][1].dtype}')\n",
    "    print(f'Validation data targets shape: {valid_dataset[1].shape} dtype: {valid_dataset[1].dtype}')\n",
    "    \n",
    "    #Create model\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    #with strategy.scope(): \n",
    "    model = get_model(hp, fold, len(train_df))\n",
    "    \n",
    "    print(model.summary())\n",
    "    print('Model prepared.')\n",
    "    \n",
    "    #Training model\n",
    "    print('Start training...')\n",
    "    filepath=f'{hp.model_weight_name}-fold{fold}.h5' #f\"best_model_fold{fold}.h5\"\n",
    "    callbacks = tf.keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                           monitor=\"val_loss\",\n",
    "                                           mode=\"min\",\n",
    "                                           save_best_only=True,\n",
    "                                           verbose=1,\n",
    "                                           save_weights_only=True)\n",
    "    history = model.fit(\n",
    "        x=train_dataset[0],\n",
    "        y=train_dataset[1],\n",
    "        validation_data=valid_dataset, \n",
    "        epochs=hp.EPOCHS,\n",
    "        shuffle=True,\n",
    "        batch_size=hp.BATCH_SIZE,\n",
    "        callbacks=[callbacks]\n",
    "    )\n",
    "    \n",
    "    valid_rmses.append(np.min(history.history['val_root_mean_squared_error']))\n",
    "    print('Training finished.')\n",
    "    del train_dataset, valid_dataset, train_df, valid_df\n",
    "    gc.collect()\n",
    "    \n",
    "print(f'{len(valid_rmses)} Folds validation RMSE:\\n{valid_rmses}')\n",
    "print(f'Local CV Average score: {np.mean(valid_rmses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n",
    "test_df.head()\n",
    "\n",
    "test_dataset = deberta_encode(test_df['full_text'])\n",
    "\n",
    "\n",
    "fold_preds = []\n",
    "for fold in range(hp.TRAIN_FOLD[0], hp.TRAIN_FOLD[1]):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = get_model(hp, fold, len(test_dataset))\n",
    "    \n",
    "    filepath=f'{hp.model_weight_name}-fold{fold}.h5' #f\"best_model_fold{fold}.h5\"\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    print(f'\\nFold {fold} inference...')\n",
    "    \n",
    "    pred = model.predict(test_dataset, batch_size=hp.BATCH_SIZE)\n",
    "    fold_preds.append(pred)\n",
    "    gc.collect()\n",
    "    \n",
    "preds = np.mean(fold_preds, axis=0)\n",
    "preds = np.clip(preds, 1, 5)\n",
    "\n",
    "sub_df = pd.concat([test_df[['text_id']], pd.DataFrame(preds, columns=TARGET_COLS)], axis=1)\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
